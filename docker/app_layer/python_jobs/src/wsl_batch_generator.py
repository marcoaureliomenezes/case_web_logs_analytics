from rand_engine.core.distinct_core import DistinctCore
from rand_engine.core.numeric_core import NumericCore
from rand_engine.core.datetime_core import DatetimeCore

from rand_engine.core.distinct_utils import DistinctUtils
from rand_engine.main.dataframe_builder import BulkRandEngine

from datetime import datetime as dt, timedelta

import faker
import csv
import os
import boto3
import logging



class WSLBatchGenerator:


  def __init__(self, logger, rand_engine):
    self.logger = logger
    self.rand_engine = rand_engine
    self.s3 = None
    self.bucket = None


  def __handle_timestamp(self, dt_end, hours_back):
    dt_start = dt_end - timedelta(hours=hours_back)
    input_format = "%Y-%m-%d %H:%M:%S"
    return input_format, dt_start.strftime(input_format), dt_end.strftime(input_format)
  

  def config_s3_client_conn(self, host, access_key, secret_key, bucket):
    self.s3 = boto3.client('s3',
      endpoint_url=host,
      aws_access_key_id=access_key,
      aws_secret_access_key=secret_key)
    self.bucket = bucket
    return self

  def config_file_prefix(self, s3_path, server, execution_date):
    partitioned_path = dt.strftime(execution_date, 'year=%Y/month=%m/day=%d')
    self.paths = {"local": f"/tmp/{server}/{partitioned_path}", "s3": f"{s3_path}/{server}/{partitioned_path}"}
    _ = os.makedirs(self.paths["local"], exist_ok=True)
    self.logger.info(f"local_path:{self.paths['local']};lake_path:{self.paths['s3']}")
    return self
  
  
  def __metadata_case_web_log_server(self, formato, dt_start, dt_end):
    # Esse método retorna um dicionário com os metadados necessários para gerar os dados usando a biblioteca rand-engine
    # Cada item do dicionário está relacionado a um campo, com exceção da chame "campos_correlacionados_proporcionais" que cria 2 campos correlacionados
    # Ao explorar as possíveis configurações desse dicionário é possível entender quais são as configurações possíveis. Mais 
    fake = faker.Faker(locale="pt_BR")
    metadata = {
      "ip_address":dict(method=DistinctCore.gen_distincts_typed, parms=dict(distinct=[fake.ipv4_public() for i in range(1000)])),
      "identificador": dict(method=DistinctCore.gen_distincts_typed, parms=dict(distinct=["-"])),
      "user": dict(method=DistinctCore.gen_distincts_typed, parms=dict(distinct=["-"])),
      "user_named": dict(method=DistinctCore.gen_distincts_typed, parms=dict(distinct=[fake.first_name().lower().replace(" ", "_") for i in range(1000)])),
      "datetime": dict(
        method=DatetimeCore.gen_datetimes, 
        parms=dict(start=dt_start, end=dt_end, format_in=formato, format_out="%d/%b/%Y:%H:%M:%S")
      ),
      "http_version": dict(
        method=DistinctCore.gen_distincts_typed,
        parms=dict(distinct=DistinctUtils.handle_distincts_lvl_1({"HTTP/1.1": 7, "HTTP/1.0": 3}, 1))
      ),
      "object_size": dict(method=NumericCore.gen_ints, parms=dict(min=0, max=10000)),
      "campos_correlacionados_proporcionais": dict(
        method=       DistinctCore.gen_distincts_typed,
        splitable=    True,
        cols=         ["http_request", "http_status"],
        sep=          ";",
        parms=        dict(distinct=DistinctUtils.handle_distincts_lvl_3({
                          "GET /home": [("200", 7),("400", 2), ("500", 1)],
                          "GET /login": [("200", 5),("400", 3), ("500", 1)],
                          "POST /login": [("201", 4),("404", 2), ("500", 1)],
                          "GET /logout": [("200", 3),("400", 1), ("400", 1)],
                          "POST /signin": [("201", 4),("404", 2), ("500", 1)],
                          "GET /balance": [("200", 3),("400", 1), ("500", 1)],
                          "POST /loans/make_loan": [("200", 3),("400", 1), ("500", 1)],
                          "GET /credit/statement.pdf": [("200", 3),("400", 1), ("500", 1)],
                          "GET /account/statement.pdf": [("200", 3),("400", 1), ("500", 1)],
                          
          }))
      )
    }
    return metadata


  def web_server_log_transformer(self, df): 
    # "This method receives the pandas dataframe generated by rand-engine and transform it."
    # Transformation 1: Decides when to give a name to the user based on authenticated endpoints;
    # Transformation 2: Transforms the resulted pandas dataframe into a series in the format of WEB_SERVER_LOGS
    authenticated_endpoints = ['GET /home', 'GET /logout', 'GET /balance', 'GET /credit/statement.pdf', 'GET /account/statement.pdf']
    associate_user_with_http_request = lambda x: x['user_named'] if x['http_request'] in authenticated_endpoints else '-'
    df['user'] = df.apply(associate_user_with_http_request, axis=1)
    df = df['ip_address'] + ' ' + df['identificador'] + ' ' + df['user'] + ' [' + df['datetime'] + ' -0300] "' + \
                        df['http_request'] + ' ' + df['http_version'] + '" ' + df['http_status'] + ' ' + df['object_size'].astype(str)
    return df



  def generate_file_name(self, dt_start, dt_end):
    dt_start, dt_end = [dt.strptime(i, "%Y-%m-%d %H:%M:%S") for i in [dt_start, dt_end]]
    start_part, end_part = dt_start.strftime("%H-%M"), dt_end.strftime("%H-%M")
    file_name = f"logs-from_{start_part}-to-{end_part}.log"
    #self.logger.info(f"File Name: {file_name}")
    return file_name


  def check_file_exists(self, path):
    try: self.s3.head_object(Bucket=self.bucket, Key=path) ; return True
    except: return False


  def write_data_in_micro_batchs(self, size, metadata, paths):
    max_size = 5*10**4
    n_parts = size // max_size
    micro_batch_size = size // n_parts
    for i in range(n_parts):
      df_pandas = self.rand_engine.create_pandas_df(metadata=metadata, size=micro_batch_size)
      df_pandas = df_pandas.sort_values(by='datetime')
      series_pandas_rand = self.web_server_log_transformer(df_pandas)
      series_pandas_rand.to_csv(paths["local"], sep=' ', index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=' ', mode='a')
    self.logger.info("Local Files Created")
    self.s3.upload_file(paths['local'], self.bucket, paths['s3'])
    self.logger.info(f"Size of local file: {os.path.getsize(paths['local'])}")
    self.logger.info("Files Uploaded to S3")



  def run(self, size_file: int, hours_back: int, execution_date: str):
    assert self.paths, "Path must be set with method config_file_path()"
    formato, dt_start, dt_end = self.__handle_timestamp(execution_date, hours_back)
    metadata = self.__metadata_case_web_log_server(formato, dt_start, dt_end)
    file_name = self.generate_file_name(dt_start, dt_end)
    total_path = {k: f"{v}/{file_name}" for k, v in self.paths.items()}
    if self.check_file_exists(total_path['s3']):
      self.logger.info(f"File {total_path['s3']} already exists in S3. Skipping...")
      return
    os.remove(total_path['local']) if os.path.exists(total_path['local']) else None
    self.write_data_in_micro_batchs(size_file, metadata, total_path)

  
  


if __name__ == "__main__":
  
  logger = logging.getLogger(__name__)
  logger.setLevel(logging.INFO)
  logger.addHandler(logging.StreamHandler())

  S3_ENDPOINT = os.getenv("S3_ENDPOINT")
  ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
  SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
  EXECUTION_DATE = os.getenv("EXECUTION_DATE", "2024-10-21 03:00:00+00:00")
  BUCKET = os.getenv("BUCKET")
  PREFIX_PATH = os.getenv("PREFIX_PATH")
  SERVER = os.getenv("SERVER")
  
  dt_execution_date = dt.strptime(EXECUTION_DATE, "%Y-%m-%d %H:%M:%S%z")
  hours_back = 6

  logger.info(f"Execution Date: {EXECUTION_DATE}")
  logger.info(f"S3 Endpoint: {S3_ENDPOINT}")
  logger.info(f"Bucket: {BUCKET}")
  logger.info(f"Prefix Path: {PREFIX_PATH}")


  rand_engine = BulkRandEngine()
  wsl_ingestor = WSLBatchGenerator(logger, rand_engine)
  
  import random
  num_rows = random.randint(7*10**5, 2*10**6)
  

  wsl_ingestor \
    .config_s3_client_conn(S3_ENDPOINT, ACCESS_KEY, SECRET_KEY, BUCKET) \
    .config_file_prefix(PREFIX_PATH, SERVER, dt_execution_date) \
    .run(num_rows, hours_back, dt_execution_date)
  
